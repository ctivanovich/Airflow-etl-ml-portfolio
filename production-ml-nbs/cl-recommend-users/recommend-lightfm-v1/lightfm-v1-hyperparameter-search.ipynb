{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import auc_score, precision_at_k, recall_at_k\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "district_id = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open(\"hyperparameters.yaml\", \"r\") as file:\n",
    "    hyperparameters = yaml.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# district_id to fraction of the users to be included\n",
    "reduced_samples = {\n",
    "    4: 0.1,\n",
    "    6: 0.8,\n",
    "    7: 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "gcs_client = storage.Client(project=\"linkbal-dp\")\n",
    "\n",
    "today = datetime.strftime(datetime.now(tz=pytz.timezone(\"Asia/Tokyo\")), \"%Y%m%d\")\n",
    "\n",
    "bucket = \"cl-personalization.datasets.linkbal.com\"\n",
    "beacon_path = \"gs://\" + bucket + \"/inputs/beacon_events/\"\n",
    "dislikes_file_base = beacon_path + \"recommend-dislikes/{}_daily_recs_dislikes.csv\"\n",
    "\n",
    "### both users and likes data are provided by the collect-recs-data DAG in Airflow\n",
    "prefix = \"inputs/lightfm\"\n",
    "users_file = \"users.csv\"\n",
    "users_path = prefix + \"/\" + users_file\n",
    "likes_file = \"likes.csv\"\n",
    "likes_path = \"gs://\" + bucket + \"/\" + prefix + \"/\" + likes_file\n",
    "\n",
    "rec_output_path = \"gs://\" + bucket + \"/outputs/lightfm_v1_district_{}.csv\"\n",
    "\n",
    "\n",
    "max_recs = 150\n",
    "target_recs = 50\n",
    "n_days_dislikes_data = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = os.cpu_count() # set this to the number of CPU cores to take advantage of parallel training\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick and dirty fix with bash utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_obj = gcs_client.get_bucket(bucket)\n",
    "bucket_obj.get_blob(f'{users_path}').download_to_filename(f'{users_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat $users_file | sed 's/\"N,/,/g' | sed 's/\"//g' > clean_users.csv\n",
    "! mv clean_users.csv $users_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cols = [\n",
    "    'user_id',\n",
    "    'gender',\n",
    "    'age',\n",
    "    'self_introduction',\n",
    "    'blood_type',\n",
    "    'brother_and_sister',\n",
    "    'job_id',\n",
    "    'annual_salary_range',\n",
    "    'body_shape',\n",
    "    'education_background',\n",
    "    'hometown_prefecture_id',\n",
    "    'nationality',\n",
    "    'holiday',\n",
    "    'smoking',\n",
    "    'drinking',\n",
    "    'housemate',\n",
    "    'sociality',\n",
    "    'intention_to_marry',\n",
    "    'marital_status',\n",
    "    'absence_or_presence_of_child',\n",
    "    'whether_want_child',\n",
    "    'housework_and_child_rearing',\n",
    "    'meeting_wish_type',\n",
    "    'first_dating_expense_type',\n",
    "    'height',\n",
    "    'personal_color',\n",
    "    'no_lover_history',\n",
    "    'gamble',\n",
    "    'cooking_skill',\n",
    "    'completion_rate',\n",
    "    'district_id',\n",
    "    'created_at'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(\n",
    "    users_file,\n",
    "    names = user_cols,\n",
    "    parse_dates = ['created_at'],\n",
    "    dtype = {'user_id': np.int32, 'target_user_id':np.int32}\n",
    ").query('district_id == @district_id').reset_index(drop = True)\n",
    "\n",
    "### reduce user space due to LightFM long run-times\n",
    "if district_id in reduced_samples:\n",
    "    user_df = user_df.sample(frac = reduced_samples[district_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define discrete variables\n",
    "continuous_vars = ['height', 'age', 'self_introduction']\n",
    "discrete_vars = list(set(user_df.columns) - set(continuous_vars) - set(['user_id','gender']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna(df, continuous_vars, discrete_vars):\n",
    "    for col in continuous_vars:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "    for col in discrete_vars:\n",
    "        df[col].fillna('na', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fillna(user_df, continuous_vars, discrete_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_age(age, low, high):\n",
    "    if age < low:\n",
    "        return 'lt_' + str(low)\n",
    "    elif age > high:\n",
    "        return 'gt_' + str(high)\n",
    "    else:\n",
    "        return str(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_df['age_cat'] = user_df.age.apply(lambda x: categorize_age(x, 18, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_height(height, low, high):\n",
    "    if height < low:\n",
    "        return 'lt_' + str(low)\n",
    "    elif height > high:\n",
    "        return 'gt_' + str(high)\n",
    "    else:\n",
    "        return str(int(height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['height_cat'] = user_df.apply(\n",
    "    lambda x: categorize_height(x.height, 150, 190) if x.gender == 1 else categorize_height(x.height, 140, 180), \n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_introduction(intro):\n",
    "    if intro == 0:\n",
    "        return 'na'\n",
    "    elif intro > 0 and intro <= 100:\n",
    "        return 'lt_100'\n",
    "    elif intro > 100 and intro <= 200:\n",
    "        return 'gt_100_lt_200'\n",
    "    elif intro > 200 and intro <= 300:\n",
    "        return 'gt_200_lt_300'\n",
    "    elif intro > 300 and intro <= 400:\n",
    "        return 'gt_300_lt_400'\n",
    "    elif intro > 400 and intro <= 500:\n",
    "        return 'gt_400_lt_500'\n",
    "    else:\n",
    "        return 'gt_500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['intro_cat']= user_df.self_introduction.apply(categorize_introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "likes_cols = [\n",
    "    'user_id',\n",
    "    'target_user_id',\n",
    "    'checked',\n",
    "    'matched',\n",
    "    'deleted'\n",
    "]\n",
    "\n",
    "like_df = pd.read_csv(\n",
    "    likes_path,\n",
    "    names = likes_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_df = like_df.drop_duplicates(subset=['user_id', 'target_user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_df = like_df[like_df.user_id.isin(user_df.user_id) & like_df.target_user_id.isin(user_df.user_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_df['checked'] = like_df['checked'].astype(bool)\n",
    "like_df['matched'] = like_df['matched'].astype(bool)\n",
    "like_df['deleted'] = like_df['deleted'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dislike data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dislikes_df(n_days=14):\n",
    "    dislikes = pd.DataFrame(\n",
    "        [], \n",
    "        columns = [\n",
    "            'user_id',\n",
    "            'target_user_id'\n",
    "        ]\n",
    "    )\n",
    "    for date in np.array([datetime.today() - timedelta(days = i) for i in range(1, n_days + 1)]):\n",
    "        try:\n",
    "            date = date.strftime(\"%Y%m%d\")\n",
    "            print(f\"Obtaining dislikes for {date}\")\n",
    "            dislikes = dislikes.append(\n",
    "                pd.read_csv(\n",
    "                    dislikes_file_base.format(date),\n",
    "                    dtype = {'user_id': np.int32, 'target_user_id':np.int32}\n",
    "                )\n",
    "            )\n",
    "            dislikes.drop_duplicates(inplace = True) \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Could not find dislikes data at {dislikes_file_base.format(date)}. Please examine URI.\")\n",
    "\n",
    "    return dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining dislikes for 20200624\n",
      "Obtaining dislikes for 20200623\n",
      "Obtaining dislikes for 20200622\n",
      "Obtaining dislikes for 20200621\n",
      "Obtaining dislikes for 20200620\n",
      "Obtaining dislikes for 20200619\n",
      "Obtaining dislikes for 20200618\n",
      "Obtaining dislikes for 20200617\n",
      "Obtaining dislikes for 20200616\n",
      "Obtaining dislikes for 20200615\n",
      "Obtaining dislikes for 20200614\n",
      "Obtaining dislikes for 20200613\n",
      "Obtaining dislikes for 20200612\n",
      "Obtaining dislikes for 20200611\n"
     ]
    }
   ],
   "source": [
    "dislike_df = create_dislikes_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dislike_df = pd.read_csv('dislikes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reducing dislikes to users and target users in the users dataset\n",
    "\n",
    "dislike_df = dislike_df[(dislike_df.user_id.isin(user_df.user_id.unique())) & (dislike_df.target_user_id.isin(user_df.user_id.unique()))]\n",
    "\n",
    "dislike_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444, 1568)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dislike_df.user_id.nunique(), user_df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Male - female matches as female - male liking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_df = like_df[like_df.matched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_df = pd.concat(\n",
    "    [\n",
    "        like_df,\n",
    "        match_df.rename(\n",
    "            columns =\n",
    "            {\n",
    "                'user_id': 'target_user_id', \n",
    "                'target_user_id': 'user_id'\n",
    "            }\n",
    "        )\n",
    "    ], \n",
    "    sort=False\n",
    ")\n",
    "\n",
    "del match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_df = like_df.drop_duplicates(subset=['user_id', 'target_user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31102"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "like_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_likes_sent = like_df.groupby('user_id').target_user_id.count().\\\n",
    "                            reset_index().\\\n",
    "                            rename(columns={\"target_user_id\": \"user_likes_sent\"})\n",
    "target_likes_received = like_df.groupby('target_user_id').user_id.count().\\\n",
    "                            reset_index().\\\n",
    "                            rename(columns={\"user_id\": \"target_likes_received\"})\n",
    "target_matches = like_df[like_df.matched].groupby('target_user_id').user_id.count().\\\n",
    "                            reset_index().\\\n",
    "                            rename(columns={\"user_id\": \"target_matches\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_df = like_df.merge(user_likes_sent, on = 'user_id', how = 'left')\n",
    "like_df = like_df.merge(target_likes_received, on='target_user_id', how = 'left')\n",
    "like_df = like_df.merge(target_matches, on='target_user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_df['liked'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = like_df.merge(dislike_df, on=['user_id', 'target_user_id'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "del like_df, user_likes_sent, target_likes_received, target_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.matched.fillna(False, inplace=True)\n",
    "merge_df.deleted.fillna(False, inplace=True)\n",
    "merge_df.checked.fillna(False, inplace=True)\n",
    "merge_df.user_likes_sent.fillna(0, inplace=True)\n",
    "merge_df.target_likes_received.fillna(0, inplace=True)\n",
    "merge_df.target_matches.fillna(0, inplace=True)\n",
    "merge_df.liked.fillna(False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The effect of this is to drop dislike records if a like was eventually sent,\n",
    "### since there should only be one interaction record b/t user and target pair, either like or dislike.\n",
    "### All dislikes duplicated have already been removed, so any duplicates remaining are 1 like and 1 dislike\n",
    "\n",
    "merge_df.drop(\n",
    "    index=merge_df[\n",
    "        (merge_df[['user_id', 'target_user_id']].duplicated(keep=False)) & \n",
    "        (merge_df.liked == 0)].index, \n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = merge_df.merge(user_df[['user_id', 'gender']], on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_interaction_df = merge_df[merge_df.gender == 1].copy()\n",
    "female_interaction_df = merge_df[merge_df.gender == 2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31958, 4631)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_interaction_df.shape[0], female_interaction_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build lightfm dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vars = [\n",
    "       'blood_type', 'brother_and_sister', 'annual_salary_range',\n",
    "       'education_background', 'holiday', 'smoking', 'drinking',\n",
    "       'sociality', 'intention_to_marry', 'marital_status',\n",
    "       'absence_or_presence_of_child', 'whether_want_child',\n",
    "       'housework_and_child_rearing', 'meeting_wish_type',\n",
    "       'first_dating_expense_type', 'body_shape', 'housemate',\n",
    "       'personal_color', 'gamble', 'cooking_skill', 'job_id',\n",
    "       'hometown_prefecture_id', 'age_cat', 'height_cat', 'intro_cat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281, 258)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creating indicator variables\n",
    "\n",
    "male_feature_list = []\n",
    "female_feature_list = []\n",
    "\n",
    "# categorical features\n",
    "for var in feature_vars:\n",
    "    male_feature_list += [var + '_' + str(val) for val in user_df[user_df.gender == 1][var].unique()]\n",
    "    female_feature_list += [var + '_' + str(val) for val in user_df[user_df.gender == 2][var].unique()]\n",
    "\n",
    "len(male_feature_list), len(female_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(x):\n",
    "    return (x['user_id'], [var + '_' + str(x[var]) for var in feature_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(interactions, weights, item_features, user_features):\n",
    "    model = LightFM(**hyperparameters)\n",
    "\n",
    "    model.fit(\n",
    "        interactions,\n",
    "        item_features=item_features,\n",
    "        user_features=user_features,\n",
    "        sample_weight=weights,\n",
    "        epochs=EPOCHS,\n",
    "        num_threads=NUM_THREADS,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Male user recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"select users who have interactions or who have completed at least half of their profiles.\"\n",
    "\n",
    "m_male_df = user_df[\n",
    "    (user_df.gender == 1) &\n",
    "    (\n",
    "        (user_df.user_id.isin(male_interaction_df.user_id.unique())) | \n",
    "        (user_df.completion_rate > 0.5)\n",
    "    )\n",
    "].copy()\n",
    "\n",
    "m_female_df = user_df[\n",
    "    (user_df.gender == 2) & \n",
    "    (\n",
    "        (user_df.user_id.isin(male_interaction_df.target_user_id.unique())) |\n",
    "        (user_df.completion_rate > 0.5)\n",
    "    )\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 686)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_male_df.shape[0], m_female_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_male_df['features'] = m_male_df.apply(create_features, axis=1)\n",
    "m_female_df['features'] = m_female_df.apply(create_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_m = Dataset()\n",
    "\n",
    "dataset_m.fit(\n",
    "    set(m_male_df['user_id']), \n",
    "    set(m_female_df['user_id']),\n",
    "    user_features=male_feature_list, \n",
    "    item_features=female_feature_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_m = dataset_m.build_user_features(m_male_df['features'])\n",
    "item_features_m = dataset_m.build_item_features(m_female_df['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate interaction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_score(x):\n",
    "    if x['matched']:\n",
    "        return 5\n",
    "    elif x['checked'] or x['deleted'] or not x['liked']:\n",
    "        return 1\n",
    "    elif x['gender'] == 2:\n",
    "        return 4\n",
    "    elif x['gender'] == 1:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_interaction_df['match_score'] = male_interaction_df.apply(match_score, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_interaction_df['tg_received_penalty'] = 1 / (np.log(male_interaction_df['target_likes_received'] + 1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_interaction_df['sent_penalty'] = 1 / (np.log(male_interaction_df['user_likes_sent'] + 1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_interaction_df['tg_generousness_score'] = np.log(male_interaction_df['target_matches'] + 1)\n",
    "male_interaction_df['tg_generousness_score'] = \\\n",
    "    (male_interaction_df['tg_generousness_score'] - male_interaction_df['tg_generousness_score'].min()) / \\\n",
    "    (male_interaction_df['tg_generousness_score'].max() - male_interaction_df['tg_generousness_score'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_MATCH_SCORE_WEIGHT = 1\n",
    "M_RECEIVED_PENALTY_WEIGHT = 2\n",
    "M_SENT_PENALTY_WEIGHT = 1\n",
    "M_TG_GENEROUSNESS_WEIGHT =  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_interaction_df['score'] = \\\n",
    "    male_interaction_df['match_score'] * M_MATCH_SCORE_WEIGHT + \\\n",
    "    male_interaction_df['tg_received_penalty'] * M_RECEIVED_PENALTY_WEIGHT + \\\n",
    "    male_interaction_df['sent_penalty'] * M_SENT_PENALTY_WEIGHT + \\\n",
    "    male_interaction_df['tg_generousness_score'] * M_TG_GENEROUSNESS_WEIGHT\n",
    "\n",
    "male_interaction_df['score'] = male_interaction_df.apply(lambda x: -1 if x.liked == 0 else x.score, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    31958.000000\n",
       "mean         1.833989\n",
       "std          1.529512\n",
       "min         -1.000000\n",
       "25%          1.601806\n",
       "50%          1.834727\n",
       "75%          2.024118\n",
       "max          6.940358\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_interaction_df.score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_interaction_df = male_interaction_df[\n",
    "    male_interaction_df.user_id.isin(m_male_df.user_id) & \n",
    "    male_interaction_df.target_user_id.isin(m_female_df.user_id)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_interactions, male_weights = dataset_m.build_interactions(\n",
    "            list(zip(male_interaction_df.user_id, male_interaction_df.target_user_id, male_interaction_df.score))\n",
    "        )\n",
    "\n",
    "m_i_train, m_i_test = random_train_test_split(male_interactions, test_percentage = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_m = train_model(m_i_train, None, item_features_m, user_features_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the effect of setting the male_weights entry to None and splitting the interactions into train and test sets has the effect of turning the below into a measure on the model's ability to predict an interaction.\n",
    "\n",
    "In Anh Khoa's original nb, he uses as a test set the set of matched interactions, but he doesn't remove it from the training set, which likely led to data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_k(\n",
    "    model_m,\n",
    "    m_i_test,\n",
    "    item_features=item_features_m,\n",
    "    user_features=user_features_m,\n",
    "    num_threads=NUM_THREADS,\n",
    "    k=50\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here I include the interaction weights as well, which leads to better prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_interactions, male_weights = dataset_m.build_interactions(\n",
    "            list(zip(male_interaction_df.user_id, male_interaction_df.target_user_id, male_interaction_df.score))\n",
    "        )\n",
    "m_i_train, m_i_test, m_w_train, m_w_test = train_test_split(male_interactions, male_weights, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m = train_model(m_i_train, m_w_train.tocoo(), item_features_m, user_features_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_k(\n",
    "    model_m,\n",
    "    m_i_test,\n",
    "    item_features=item_features_m,\n",
    "    user_features=user_features_m,\n",
    "    num_threads=NUM_THREADS,\n",
    "    k=50\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here I do what Khoa-san intended, by only testing on matched interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = male_interaction_df[male_interaction_df['matched']]\n",
    "\n",
    "male_interactions, male_weights = dataset_m.build_interactions(\n",
    "            list(zip(matched.user_id, matched.target_user_id, matched.score))\n",
    "        )\n",
    "m_i_train, m_i_test, m_w_train, m_w_test = train_test_split(male_interactions, male_weights, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m = train_model(m_i_train, m_w_train.tocoo(), item_features_m, user_features_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_k(\n",
    "    model_m,\n",
    "    m_i_test,\n",
    "    item_features=item_features_m,\n",
    "    user_features=user_features_m,\n",
    "    num_threads=NUM_THREADS,\n",
    "    k=50\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sample_hyperparameters():\n",
    "    \"\"\"\n",
    "    Yield possible hyperparameter choices.\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        yield {\n",
    "            \"no_components\": np.random.randint(50, 150),\n",
    "            \"learning_schedule\": np.random.choice([\"adagrad\", \"adadelta\"]),\n",
    "            \"loss\": np.random.choice([\"warp\", \"bpr\"]),\n",
    "            \"learning_rate\": np.random.exponential(0.05),\n",
    "            \"max_sampled\": np.random.randint(5, 15),\n",
    "            \"num_epochs\": np.random.randint(50, 150),\n",
    "        }\n",
    "\n",
    "\n",
    "def random_search(i_train, i_test, w_train, w_test, dwriter, num_samples=10):\n",
    "    \"\"\"\n",
    "    Sample random hyperparameters, fit a LightFM model, and evaluate it\n",
    "    on the test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    train: np.float32 coo_matrix of shape [n_users, n_items]\n",
    "        Training data.\n",
    "    test: np.float32 coo_matrix of shape [n_users, n_items]\n",
    "        Test data.\n",
    "    num_samples: int, optional\n",
    "        Number of hyperparameter choices to evaluate.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    generator of (auc_score, hyperparameter dict, fitted model)\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(num_samples):\n",
    "        hyperparams = sample_hyperparameters().__next__()\n",
    "        print(\"---Training with hyperparameters---\")\n",
    "        pprint(hyperparams)\n",
    "        num_epochs = hyperparams.pop(\"num_epochs\")\n",
    "\n",
    "        model = LightFM(**hyperparams)\n",
    "        start = time.time()\n",
    "        model.fit(\n",
    "            i_train,\n",
    "            item_features=item_features_m,\n",
    "            user_features=user_features_m,\n",
    "            sample_weight=w_train,\n",
    "            verbose=False,\n",
    "            epochs=num_epochs, \n",
    "            num_threads=NUM_THREADS\n",
    "        )\n",
    "        train_time = int(time.time() - start)\n",
    "        score = precision_at_k(\n",
    "            model, \n",
    "            test_interactions=m_i_test,\n",
    "            train_interactions=m_i_train, \n",
    "            user_features = user_features_m,\n",
    "            item_features = item_features_m,\n",
    "            k = 50,\n",
    "            num_threads=NUM_THREADS,\n",
    "            check_intersections=False\n",
    "            ).mean()\n",
    "\n",
    "        hyperparams[\"num_epochs\"] = num_epochs\n",
    "        hyperparams[\"train_time\"] = train_time\n",
    "        hyperparams[\"score\"] = score\n",
    "        dwriter.writerow(hyperparams)\n",
    "        yield (score, hyperparams, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hyperparameters-warp-100.csv\", \"w\") as outcsv, open(\"hyperparameters.yaml\", \"r\") as hypfile:\n",
    "    dwriter = csv.DictWriter(\n",
    "        outcsv, \n",
    "        fieldnames=[\n",
    "            \"num_epochs\",\n",
    "            \"train_time\",\n",
    "            \"mar@k\",\n",
    "            \"map@k\",\n",
    "            \"auc\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    dwriter.writeheader()\n",
    "\n",
    "    hyperparams = yaml.load(hypfile)\n",
    "    \n",
    "    for num_epochs in range(50, 151, 10):\n",
    "        csv_output = {}\n",
    "        \n",
    "        male_interactions, male_weights = dataset_m.build_interactions(\n",
    "            list(zip(male_interaction_df.user_id, male_interaction_df.target_user_id, male_interaction_df.score))\n",
    "        )\n",
    "\n",
    "        m_i_train, m_i_test, m_w_train, m_w_test = train_test_split(male_interactions, male_weights, test_size = 0.33)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        model = LightFM(**hyperparams)\n",
    "        model.fit(\n",
    "            m_i_train,\n",
    "            item_features=item_features_m,\n",
    "            user_features=user_features_m,\n",
    "            sample_weight=m_w_train.tocoo(),\n",
    "            verbose=False,\n",
    "            epochs=num_epochs, \n",
    "            num_threads=NUM_THREADS\n",
    "        )\n",
    "\n",
    "        train_time = int(time.time() - start)\n",
    "\n",
    "        mapk = precision_at_k(\n",
    "            model, \n",
    "            test_interactions=m_i_test,\n",
    "            train_interactions=m_i_train, \n",
    "            user_features = user_features_m,\n",
    "            item_features = item_features_m,\n",
    "            k = 50,\n",
    "            num_threads=NUM_THREADS,\n",
    "            check_intersections=False\n",
    "        ).mean()\n",
    "        mark = recall_at_k(\n",
    "            model, \n",
    "            test_interactions=m_i_test,\n",
    "            train_interactions=m_i_train, \n",
    "            user_features = user_features_m,\n",
    "            item_features = item_features_m,\n",
    "            k = 50,\n",
    "            num_threads=NUM_THREADS,\n",
    "            check_intersections=False\n",
    "        ).mean()\n",
    "        auc = auc_score(\n",
    "            model,\n",
    "            test_interactions=m_i_test,\n",
    "            train_interactions=m_i_train,\n",
    "            user_features=user_features_m,\n",
    "            item_features=item_features_m,\n",
    "            preserve_rows=False,\n",
    "            num_threads=NUM_THREADS,\n",
    "            check_intersections=False\n",
    "        ).mean()\n",
    "\n",
    "        csv_output[\"num_epochs\"] = num_epochs\n",
    "        csv_output[\"train_time\"] = train_time\n",
    "        csv_output[\"mar@k\"] = mark\n",
    "        csv_output[\"map@k\"] = mapk\n",
    "        csv_output[\"auc\"] = auc\n",
    "\n",
    "        dwriter.writerow(csv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
