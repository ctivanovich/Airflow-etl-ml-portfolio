{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data collection and transformation\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from aiohttp import ClientSession\n",
    "from io import BytesIO\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "\n",
    "### Neural network, cropping and delivering imports\n",
    "import os, sys, shutil\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, training\n",
    "from scripts.hd5_dataset import HD5Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "### Parameter Cell\n",
    "DISTRICT_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables\n",
    "BASE_URL = \"https://..../{user_id}/{file_name}\"\n",
    "BUCKET_BASE = \"...\"\n",
    "PREFIX = \"inputs/image-clustering\"\n",
    "METADATASOURCE = \"/inputs/image-clustering/image_metadata_ver_01.csv\"\n",
    "\n",
    "### I decided to write files to the host filesystem, as they get deleted in the end, and as we really \n",
    "### need to optimize to avoid memory overflow\n",
    "BASE_DIR = \".\"\n",
    "\n",
    "CROPPED_DIR = BASE_DIR + f\"/cropped_{DISTRICT_ID}/\"\n",
    "IMAGES_H5 = BASE_DIR + f\"/images_{DISTRICT_ID}.h5\"\n",
    "IDENTIFIERS_H5 = BASE_DIR + f\"/identifiers_{DISTRICT_ID}.h5\"\n",
    "\n",
    "images = []\n",
    "identifiers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\n",
    "    \"gs://\" + BUCKET_BASE + METADATASOURCE,\n",
    "    names = [\n",
    "        'user_id',\n",
    "        'filename',\n",
    "        'gender',\n",
    "        'district_id'\n",
    "    ]\n",
    ").query(\"district_id == @DISTRICT_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metadata.shape[0] == 0:\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking and removing entries that have already been obtained, cropped, and stored in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_stored_images = [\n",
    "    re.search(r\"(\\d/[0-9]{6,8}\\/.+.jpg)$\", blob.name)[0].split('/') for blob in client.list_blobs(\n",
    "        bucket_or_name=BUCKET_BASE,\n",
    "        prefix=PREFIX\n",
    "    ) if 'jpg' in blob.name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored = pd.DataFrame(already_stored_images, columns=['gender', 'user_id', 'filename'])\n",
    "stored.user_id = stored.user_id.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of unobtained images is: {metadata[~metadata.filename.isin(stored.filename)].shape[0]}\")\n",
    "metadata = metadata[~metadata.filename.isin(stored.filename)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain images via asynchronous scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_and_meta_generator(meta_data):\n",
    "    for uid, filename, gender, _ in meta_data:\n",
    "        url = BASE_URL.format(\n",
    "            user_id = uid,\n",
    "            file_name = filename\n",
    "            )\n",
    "        yield uid, filename, gender, url\n",
    "\n",
    "async def get_image_data(url: str, session: ClientSession, **kwargs) -> bytes:\n",
    "    \"\"\"GET request wrapper to fetch image bytes array data.\n",
    "\n",
    "    kwargs are passed to `session.request()`.\n",
    "    \"\"\"\n",
    "    resp = await session.request(method=\"GET\", url=url, **kwargs)\n",
    "    content = await resp.read() #read for binary, text() for text\n",
    "    await asyncio.sleep(0.1)\n",
    "    return content\n",
    "\n",
    "async def convert_and_append_images(\n",
    "    uid: int, \n",
    "    filename:str,\n",
    "    gender:int,\n",
    "    url:str, \n",
    "    session: ClientSession, \n",
    "    **kwargs\n",
    "    ) -> None:\n",
    "    \"\"\"Obtain image bytes data, append as flatted array of dim (600 * 600 * 3, 1). \n",
    "    Append user ID to separate list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bytes_content = await get_image_data(url=url, session=session, **kwargs)\n",
    "        rgb_bytes_array = Image.open(BytesIO(bytes_content))\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    else:\n",
    "        identifier = str(gender) + '/' + str(uid) + '/' + filename\n",
    "        identifiers.append(identifier)\n",
    "        rgb_numeric_array = np.asarray(rgb_bytes_array, dtype = 'uint8')\n",
    "        flattened_rgb = rgb_numeric_array.flatten()\n",
    "        images.append(flattened_rgb)\n",
    "\n",
    "async def main(meta_data):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for uid, filename, gender, url in url_and_meta_generator(meta_data):\n",
    "            tasks.append(asyncio.create_task(convert_and_append_images(uid, filename, gender, url, session)))\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await main(metadata.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Storage for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_many_hdf5(images, identifiers):\n",
    "    \"\"\" Stores an array of images to HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        images       image arrays, n_images * (600, 600, 3) to be stored\n",
    "        labels       labels arrays, n_images * 1 to be stored\n",
    "    \"\"\"\n",
    "    # Create a new HDF5 file\n",
    "    # Hierarchy: File -> Group -> Dataset -> actual numpy array of image data\n",
    "\n",
    "    data = h5py.File(IMAGES_H5, \"w\")\n",
    "    meta = h5py.File(IDENTIFIERS_H5, \"w\")\n",
    "    try:\n",
    "        data.create_dataset(\n",
    "            f\"{DISTRICT_ID}\",\n",
    "            images.shape,\n",
    "            h5py.h5t.STD_U8BE,\n",
    "            data = images\n",
    "        )\n",
    "        meta.create_dataset(\n",
    "            f\"{DISTRICT_ID}\",\n",
    "            identifiers.shape,\n",
    "            h5py.special_dtype(vlen=str),\n",
    "            data = identifiers\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        data.close()\n",
    "        meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "identifiers = np.array(identifiers, dtype = 'S')\n",
    "\n",
    "store_many_hdf5(images, identifiers)\n",
    "\n",
    "print(identifiers[0])\n",
    "del images, identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop faces to Facenet's preferred dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CROPPED_DIR):\n",
    "    os.mkdir(CROPPED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 8\n",
    "# workers = 0 if os.name == 'nt' else 8\n",
    "dim = 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=dim,\n",
    "    margin=0,\n",
    "    min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], \n",
    "    factor=0.709, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = \".\"\n",
    "hd5data = HD5Dataset(\n",
    "    data_path = IMAGES_H5, \n",
    "    dataset_label = f'{DISTRICT_ID}',\n",
    "    indices_path = IDENTIFIERS_H5,\n",
    "    indices_label = f'{DISTRICT_ID}',\n",
    "    transforms = transforms.Resize((300)), \n",
    "    image_dims = (600,600,3)\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    hd5data,\n",
    "    collate_fn=training.collate_pil\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (img, label) in enumerate(loader):\n",
    "    mtcnn(img[0], save_path =  CROPPED_DIR + label[0])\n",
    "    print('\\rImage {} of {}'.format(i + 1, len(loader)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd5data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Upload to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = client.get_bucket(BUCKET_BASE)\n",
    "\n",
    "def upload_cropped_image_to_gcs(bucket, file, blob_name):\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(file, content_type='image/jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in glob.glob(f'{CROPPED_DIR}/*/*/*.jp*g'):\n",
    "    base = \"inputs/image-clustering/cropped_faces/\"\n",
    "    filename = re.search(r\"cropped_[0-9]{1,2}/(.*)\", path).group(1)\n",
    "    blob_name = (base + filename) #.replace('\\\\', '/') #only needed for windows\n",
    "    try:\n",
    "        print(blob_name)\n",
    "        upload_cropped_image_to_gcs(\n",
    "            bucket,\n",
    "            file = path,\n",
    "            blob_name = blob_name\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"{file} has failed with {e}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up local files and cropped directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(IMAGES_H5)\n",
    "os.remove(IDENTIFIERS_H5)\n",
    "shutil.rmtree(CROPPED_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}