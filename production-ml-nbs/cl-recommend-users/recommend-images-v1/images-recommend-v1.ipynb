{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, shutil\n",
    "import torch\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import storage\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, utils\n",
    "\n",
    "from facenet_pytorch.models import inception_resnet_v1\n",
    "from facenet_pytorch import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "### Parameterization\n",
    "DISTRICT_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Local FS Declarations\n",
    "BASE_DIR = \".\"\n",
    "CROPPED_DIR = BASE_DIR + f'/cropped_{DISTRICT_ID}'\n",
    "\n",
    "### GCS Declaratiosns\n",
    "BUCKET = \"....\"\n",
    "INPUTS = \"inputs/image-clustering/\"\n",
    "CROPPED_IMAGES = \"cropped_faces/\"\n",
    "MODEL_STATE_DICT = \"model_state_dict.pkl\"\n",
    "OUTPUT_URI = f\"/outputs/image_clustering_v1_district_{DISTRICT_ID}.csv\"\n",
    "\n",
    "### Inpute GCS objects\n",
    "users_file = \"gs://.../inputs/users.csv\"\n",
    "likes_file = \"gs://.../inputs/implicit-svd/likes_v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "### CNN Embedding learning: because the model has already been trained, the importance of this is largely for memory management\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "### Nearest neighbors and recs\n",
    "NNEIGHBORS = 300\n",
    "NRECS = 150\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client('....')\n",
    "bucket = client.get_bucket(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Likes data for use in Nearest Neighbors Search and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_cols = [\n",
    "    '....'\n",
    "]\n",
    "\n",
    "likes_df = pd.read_csv(\n",
    "    likes_file,\n",
    "    names = likes_cols\n",
    ").query(\"(user_district_id == @DISTRICT_ID) and (target_district_id == @DISTRICT_ID)\")\n",
    "\n",
    "users = pd.read_csv(\n",
    "    users_file, \n",
    "    header = None,\n",
    "    names = [\n",
    "        'user_id',\n",
    "        'gender',\n",
    "        'district_id'\n",
    "    ]\n",
    ").query(\"district_id == @DISTRICT_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_df = likes_df[likes_df.user_id.isin(users['user_id'].unique())]\n",
    "likes_df.user_id.nunique(), users.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above is a sanity check, to make sure that the likes data corresponds with our master users list; simply, if someone liked someone and entered the likes data, we expect them also to be in the users list. I don't expect a 1-to-1 correspondence, but the closer the better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining cropped photos\n",
    "\n",
    "Structure is simply /user_id/photos.jpg in GCS.\n",
    "\n",
    "However, we want to reduce the images we will use to those in the users.csv file, to avoid providing recs with inactive users. So, locally, we only download users for this district, and these are the only images we will use as a basis for recommendation, together with likes data to find nearest neighbors.\n",
    "\n",
    "Inclusion criteria have a major impact on interpretability of results, so here they are, at least for this model and the data inputs:\n",
    "From users.sql:\n",
    "\n",
    "```sql\n",
    "Redacted: custom business conditions\n",
    "```\n",
    "These match the likes.sql, though the likes.sql also screens for new users (account less than 1 day old). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_blobs = [b for b in bucket.list_blobs(prefix = INPUTS + CROPPED_IMAGES)  if '.jpg' in b.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(users.user_id.unique()).intersection(set([int(b.name.split('/')[4]) for b in img_blobs])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tells us how many likes.target_users we have images for. These are the people that we can recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CROPPED_DIR):\n",
    "    os.mkdir(CROPPED_DIR)\n",
    "    os.mkdir(CROPPED_DIR + '/1')\n",
    "    os.mkdir(CROPPED_DIR + '/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images_downloaded = 0\n",
    "\n",
    "for img_blob in img_blobs:\n",
    "    groups = re.search(r\"cropped_faces/(\\d)/(.*)/(.*\\.jpg)$\", img_blob.name)\n",
    "    if int(groups.group(2)) in users.user_id.unique():\n",
    "        userdir = CROPPED_DIR + \"/\" + groups.group(1) + \"/\" + groups.group(2) + \"/\"\n",
    "        filename = groups.group(3)\n",
    "        if not os.path.exists(userdir):\n",
    "            os.makedirs(userdir)\n",
    "        if not os.path.exists(userdir + filename):\n",
    "            img_blob.download_to_filename(userdir + filename)\n",
    "            n_images_downloaded += 1\n",
    "\n",
    "n_images_downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data transformation, custom dataset, batch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prewhiten(x):\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    std_adj = std.clamp(min=1.0/(float(x.numel())**0.5))\n",
    "    y = (x - mean) / std_adj\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(182),\n",
    "        transforms.ToTensor(),\n",
    "        prewhiten,\n",
    "        transforms.Normalize(\n",
    "            mean = (0.485, 0.456, 0.406), \n",
    "            std = (0.229, 0.224, 0.225)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimages = datasets.ImageFolder(\n",
    "    CROPPED_DIR + \"/1\",\n",
    "    transform = preprocess\n",
    ")\n",
    "\n",
    "fimages = datasets.ImageFolder(\n",
    "    CROPPED_DIR + \"/2\",\n",
    "    transform = preprocess\n",
    ")\n",
    "\n",
    "### and because we need a mapping of dataset indexing to class (target user id)\n",
    "f_idx_to_class = {ix:tid for ix, tid in zip(fimages.class_to_idx.values(), fimages.class_to_idx.keys())}\n",
    "m_idx_to_class = {ix:tid for ix, tid in zip(mimages.class_to_idx.values(), mimages.class_to_idx.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(dataset):\n",
    "    return DataLoader(\n",
    "        dataset=dataset,\n",
    "        ### condition is needed for cases where size of image dataset is less than a standard batch\n",
    "        batch_size=BATCH_SIZE if len(dataset) > BATCH_SIZE else len(dataset), \n",
    "        shuffle=True, ### previously, i had this set to true; but \n",
    "        drop_last=False #\n",
    "        ## the effect of setting this true is to drop the last batch if it is less than batch_size\n",
    "        ### this is necessary if batch normalization is used, but it should be turned off for inference as we do here\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \"Facenet\" Inception-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_embedding(model, dataset, mapping):\n",
    "    model.to(device);\n",
    "    model.eval();\n",
    "    output = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in create_loader(dataset):\n",
    "            out = model(batch[0].to(device))\n",
    "            output.append(out.detach().cpu().numpy()) #necessary for later processing on CPU and to avoid killing GPU\n",
    "            labels.append([mapping[int(ix)] for ix in batch[1]])\n",
    "    output = np.concatenate(np.array(output))\n",
    "    labels = np.concatenate(np.array(labels))\n",
    "    return output, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and TSNE for dimensionality reduction and separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(n_components, embedding):\n",
    "    pca = PCA(n_components = n_components)\n",
    "    reduced = pca.fit_transform(embedding)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_tsne(p, reduced):\n",
    "    tsne = TSNE(\n",
    "        perplexity=p,\n",
    "        learning_rate=200,\n",
    "        n_iter=700,\n",
    "        verbose=0,\n",
    "        n_components=3\n",
    "    )\n",
    "    out = tsne.fit_transform(reduced)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering function with CL-side conditions to avoid serving up users who will be filtered out by the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_items(user_id, likes):\n",
    "    \"\"\"\n",
    "    Apply business-related filtering conditions\n",
    "    \"\"\"\n",
    "    user_age = likes[likes.user_id == user_id].user_age.iloc[0]\n",
    "\n",
    "    filtered_target_users = likes[ \n",
    "      ('') |\n",
    "      ('') |\n",
    "      ('') # find users previously liked by this user; this is necessary, unlike with implicit\n",
    "    ].target_user_id.unique()\n",
    "    \n",
    "    return filtered_target_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendation generation with N target neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recs(embedding_df, likes_):\n",
    "    \"\"\"\n",
    "    Rec generation. We pass target_n as the number of recs, which matches the number of neighbors we want from the NN model.\n",
    "    The reason for this is to ensure we generate enough recs if someone only has 1 like in their history. From that 1 like,\n",
    "    we need to infer N neighors, and from those N neighbors, we need to make sure we can produce N recs after filtering.\n",
    "    \n",
    "    If a user has many liked target users, then we take a random sample of size Sample, collect N neighbors for each sampled target \n",
    "    user, and iterate through them in order of distance until we have enough new recs to provide after filtering them.\n",
    "    \"\"\"\n",
    "    recommendations = {}\n",
    "    n_potential_targets = embedding_df.shape[0]\n",
    "    # set nneighbors and fit model\n",
    "    neighbors_model = NearestNeighbors(n_neighbors = NNEIGHBORS if NNEIGHBORS < n_potential_targets else n_potential_targets)\n",
    "    neighbors_model.fit(embedding_df[['x', 'y', 'z']])\n",
    "    \n",
    "    for user_id in likes_.user_id.unique():\n",
    "        neighbors = []\n",
    "        recommendations[user_id] = []\n",
    "        n_recs = 0\n",
    "        \n",
    "        to_be_filtered = filter_items(user_id, likes_)\n",
    "\n",
    "        liked_users = likes_[likes_.user_id == user_id].target_user_id.values\n",
    "        \n",
    "        ### we take a subset of liked users for those who have liked more than NNEIGHBORS, \n",
    "        ### so this becomes at most NNEIGHBORS x NNEIGHBORS obtained for a user\n",
    "        shuffled_subset = np.random.permutation(liked_users)[:NNEIGHBORS]\n",
    "        \n",
    "        ### here, we iterate through previously liked users, and obtain neighbors\n",
    "        for liked_user in shuffled_subset:\n",
    "            k_neighbors_ix = neighbors_model.kneighbors(\n",
    "                embedding_df[embedding_df.target_user_id == liked_user][['x','y','z']],\n",
    "                return_distance=False\n",
    "            )[0]\n",
    "            k_neighbors = embedding_df.iloc[k_neighbors_ix].target_user_id.values\n",
    "            neighbors.append(k_neighbors)\n",
    "        neighbors = np.stack(neighbors)\n",
    "        \n",
    "        for j in range(neighbors.shape[1]):\n",
    "            ### iterating through the columns of the matrix, because position j corresponds to k\n",
    "            ### in terms of nearness\n",
    "            for neighbor in neighbors[1:, j]: ### we index 1: because first neighbor is in fact the liked_user\n",
    "                ### neighbor not in to be filtered\n",
    "                ### neighbor in likes data, meaning this person has been active within 2 weeks\n",
    "                ### neighbor not duplicated as a rec\n",
    "                if neighbor not in to_be_filtered \\\n",
    "                and neighbor not in recommendations[user_id]:\n",
    "                    recommendations[user_id].append(neighbor)\n",
    "                    n_recs += 1\n",
    "                    if n_recs == NRECS:\n",
    "                        break\n",
    "            if n_recs == NRECS:\n",
    "                break\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, dataset, likes_, mapping, perplexity = 30):\n",
    "\n",
    "    embedding, labels = generate_features_embedding(model, dataset, mapping)\n",
    "    \n",
    "    pca_reduced = do_PCA(\n",
    "        ### condition is needed for cases where size of image dataset is less than 50\n",
    "        ### note: for the data loader, the last incomplete batch is dropped\n",
    "        n_components = 50 if len(embedding) > 50 else len(embedding), \n",
    "        embedding = embedding\n",
    "    )\n",
    "    \n",
    "    tsne_reduced = do_tsne(perplexity, pca_reduced)\n",
    "    \n",
    "    embedding_df = pd.DataFrame(\n",
    "        data = tsne_reduced,\n",
    "        columns = [\n",
    "            'x',\n",
    "            'y',\n",
    "            'z'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    embedding_df[\"target_user_id\"] = labels\n",
    "    embedding_df[\"target_user_id\"] = embedding_df.target_user_id.astype(int)\n",
    "    \n",
    "    print(f\"Average number of face photos per target user is {embedding_df.groupby('target_user_id').x.count().mean()}\")\n",
    "    \n",
    "    #Reduce target users to those we have images for, since we need images to make recommendations\n",
    "    likes_ = likes_[likes_.target_user_id.isin(labels)]\n",
    "    \n",
    "    print(\"N unique for likes data with condition of being in photos data: \\n\")\n",
    "    print(likes_[['user_id', 'target_user_id']].nunique())\n",
    "    \n",
    "    recommendations = generate_recs(embedding_df, likes_)\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining and loading model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{BASE_DIR}/model_state_dict.pkl\"):\n",
    "    blob = bucket.get_blob(INPUTS + MODEL_STATE_DICT)\n",
    "    blob.download_to_filename(f\"{BASE_DIR}/model_state_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv1 = inception_resnet_v1.InceptionResnetV1(pretrained='vggface2', num_classes = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(f'{BASE_DIR}/model_state_dict.pkl', map_location=torch.device('cpu') if device == 'cpu' else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv1.load_state_dict(model_state_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating for men"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subset the likes data to only include likes with target_gender equal to the opposite gender of the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_recs = main(\n",
    "    model = inceptionv1, \n",
    "    dataset = fimages, \n",
    "    likes_ = likes_df[likes_df.target_gender == 2], ### Reduce target users to the gender we are recommending\n",
    "    mapping = f_idx_to_class\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = pd.DataFrame(\n",
    "    data = [[user_id, rec] for user_id in male_recs for rec in male_recs[user_id]],\n",
    "    columns = [\"user_id\", \"recommended_user_id\"]\n",
    "    ).sort_values(\n",
    "        by = \"user_id\"\n",
    "    )\n",
    "\n",
    "del male_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.groupby('user_id').count().recommended_user_id.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating for women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_recs = main(\n",
    "    model = inceptionv1, \n",
    "    dataset = mimages, \n",
    "    likes_ = likes_df[likes_df.target_gender == 1], ### Reduce target users to the gender we are recommending\n",
    "    mapping = m_idx_to_class\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = pd.DataFrame(\n",
    "    data = [[user_id, rec] for user_id in female_recs for rec in female_recs[user_id]],\n",
    "    columns = [\"user_id\", \"recommended_user_id\"]\n",
    "    ).sort_values(\n",
    "        by = \"user_id\"\n",
    "    )\n",
    "\n",
    "del female_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.groupby('user_id').count().recommended_user_id.plot(kind = 'hist', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and deliver to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_m, df_f], ignore_index = True)\n",
    "df_concat.to_csv(\"gs://\" + BUCKET + OUTPUT_URI, header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up of all downloaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(CROPPED_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}