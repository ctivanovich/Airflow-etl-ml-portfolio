{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalization V2: Collaborative Filtering on Likes and Views\n",
    "\n",
    "\n",
    "### [Obtaining Views and Likes Data](#Obtaining-Views-and-Likes-Data)\n",
    "\n",
    "### [Transforming and Aggregating Data](#Transforming-and-Aggregating-Data)\n",
    "\n",
    "### [Creating Score and Applying Penalties](#Creating-Score-and-Applying-Penalties)\n",
    "\n",
    "### [Preparing Data](#Preparing-Data)\n",
    "\n",
    "### [Training Model](#Training-Model)\n",
    "\n",
    "### [Selecting Daily Test Users](#Selecting-Daily-Test-Users)\n",
    "\n",
    "### [Generating Predictions](#Generating-Predictions)\n",
    "\n",
    "### [Writing Out Predictions](#Writing-Out-Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from google.cloud import storage\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "today = datetime.strftime(datetime.now(tz=pytz.timezone(\"Asia/Tokyo\")), \"%Y%m%d\")\n",
    "\n",
    "bucket = \"...\"\n",
    "\n",
    "in_bucket_base = bucket + \"inputs/\"\n",
    "in_bucket = bucket + \"inputs/implicit-svd/\"\n",
    "out_bucket = bucket + \"outputs/\"\n",
    "\n",
    "dislikes_bucket = in_bucket_base + \"beacon_events/recommend-dislikes/\"\n",
    "dislikes_file_base = dislikes_bucket + \"{}_daily_recs_dislikes.csv\"\n",
    "\n",
    "### both users and likes data are provided by the collect-recs-data DAG in Airflow\n",
    "users_file = in_bucket_base + \"users.csv\"\n",
    "likes_file = in_bucket + \"likes_v2.csv\"\n",
    "\n",
    "N_recs = 100\n",
    "N_DAYS_DISLIKES_DATA = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papermill Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "district_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"implicit_svd_v2_district_{district_id}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring Input/Output GCS Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client(project=\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining user sample group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(users_file, names = ['user_id', 'gender', 'district_id'])\n",
    "users = users[users.district_id == district_id]\n",
    "users = users[['user_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likes Data (SQL)\n",
    "\n",
    "The following code is intended for automated implementation on a daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_cols = [\n",
    "    'user_id',\n",
    "    'gender',\n",
    "    'user_age',\n",
    "    'user_prefecture_id',\n",
    "    'user_district_id',\n",
    "    'target_user_id',\n",
    "    'target_gender',\n",
    "    'target_user_age',\n",
    "    'target_prefecture_id',\n",
    "    'target_district_id',\n",
    "    'has_greeting',\n",
    "    'matched'\n",
    "]\n",
    "\n",
    "likes = pd.read_csv(likes_file, names = likes_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering likes to the 'district'\n",
    "\n",
    "As this is a factorization method, we cannot train the model and then remove users later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes = likes[likes.user_id.isin(users['user_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes.query('gender == 1').user_id.nunique(), likes.query('gender == 2').user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering matched likes as likes from the target_users' perspectives\n",
    "Apparently, liking in response to a received like is not separately recorded as a like. This means the majority of likes from our female users are not logged as such in the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proportion of matches with women (passive likes) and sent out by women (active likes)\")\n",
    "((likes.matched == True) & (likes.target_gender == 2)).mean(), (likes.gender == 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for women\n",
    "matched_likes_for_women_bef = likes[(likes.matched == True) & (likes.target_gender == 2)]\n",
    "\n",
    "matched_likes_for_women = pd.DataFrame({\n",
    "  'user_id': matched_likes_for_women_bef['target_user_id'],\n",
    "  'gender': matched_likes_for_women_bef['target_gender'],\n",
    "  'user_age': matched_likes_for_women_bef['target_user_age'],\n",
    "  'user_district_id': matched_likes_for_women_bef['target_district_id'],\n",
    "  'target_user_id': matched_likes_for_women_bef['user_id'],\n",
    "  'target_gender': matched_likes_for_women_bef['gender'],\n",
    "  'target_user_age': matched_likes_for_women_bef['user_age'],\n",
    "  'target_district_id': matched_likes_for_women_bef['user_district_id'],\n",
    "  'has_greeting': 0, # 一応0にしました\n",
    "  'matched': 0 # 一応0にしました\n",
    "})\n",
    "\n",
    "likes = likes.append(matched_likes_for_women)\n",
    "\n",
    "# Filtering again, because we added new likes data\n",
    "likes = likes[likes.user_id.isin(users['user_id'].unique())]\n",
    "\n",
    "# In case of instances where likes get duplicated, this drops on user_id, target_user_id being duplicated, and keeps only\n",
    "# the first entry. Each user_id -> target_user_id pair should be unique.\n",
    "likes.drop(likes[likes[['user_id', 'target_user_id']].duplicated(keep = False)].index, axis = 0, inplace = True)\n",
    "\n",
    "del matched_likes_for_women_bef, matched_likes_for_women"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming and Aggregating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_like_counts = likes.groupby('user_id').target_user_id.count()\n",
    "target_like_counts = likes.groupby('target_user_id').user_id.count()\n",
    "user_like_counts.name = 'user_likes_sent'\n",
    "target_like_counts.name = 'target_likes_received'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With > 1 likes the entry requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion of target_user_gender be removed and impact on likes data set size\n",
    "print((user_like_counts > 1).mean())\n",
    "print(likes[likes.user_id.isin(user_like_counts[user_like_counts > 1].index)].shape[0]/likes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = likes[likes.user_id.isin(user_like_counts[user_like_counts > 1].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"liked\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Obtaining the past 2 weeks of DailyRecommend Dislikes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dislikes_df(n_days):\n",
    "    dislikes = pd.DataFrame(\n",
    "        [], \n",
    "        columns = [\n",
    "            'user_id',\n",
    "            'target_user_id'\n",
    "        ]\n",
    "    )\n",
    "    for date in np.array([datetime.today() - timedelta(days = i) for i in range(1, n_days + 1)]):\n",
    "        try:\n",
    "            date = date.strftime(\"%Y%m%d\")\n",
    "            print(f\"Obtaining dislikes for {date}\")\n",
    "            dislikes = dislikes.append(\n",
    "                pd.read_csv(\n",
    "                    dislikes_file_base.format(date),\n",
    "                    dtype = {'user_id': np.int32, 'target_user_id':np.int32}\n",
    "                )\n",
    "            )\n",
    "            dislikes.drop_duplicates(inplace = True) \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Could not find dislikes data at {dislikes_file_base.format(date)}. Please examine URI.\")\n",
    "        \n",
    "    dislikes['liked'] = 0\n",
    "\n",
    "    return dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dislikes = create_dislikes_df(n_days = N_DAYS_DISLIKES_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dislikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reducing dislikes to users and target users in the likes dataset to avoid conflicts\n",
    "### it's possible to avoid this step (if the dislikes were also subset for district, \n",
    "### but the effect of avoiding it would be to introduce new user-target_user pairs, who have not liked.\n",
    "### this might not be a bad thing, but it would require re-thinking how to weight negative interactions\n",
    "### together with any other interaction data for these interactions\n",
    "\n",
    "dislikes = dislikes[\n",
    "    (dislikes.user_id.isin(df.user_id.unique())) & \n",
    "    (dislikes.target_user_id.isin(df.target_user_id.unique()))\n",
    "]\n",
    "\n",
    "dislikes.drop_duplicates(inplace=True)\n",
    "dislikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_gender = likes.set_index('user_id').gender.to_dict()\n",
    "target_user_gender = likes.set_index('target_user_id').target_gender.to_dict()\n",
    "\n",
    "dislikes['gender'] = dislikes.user_id.apply(lambda x: user_gender[x])\n",
    "dislikes['target_gender'] = dislikes.target_user_id.apply(lambda x: target_user_gender[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_gender, target_user_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is check on disliked user-target_user pairs also being in the likes data as liked pairs.\n",
    "\n",
    "Resetting the index first is necessary for the .isin() function to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True, inplace = True)\n",
    "dislikes.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[['user_id', 'target_user_id']].isin(dislikes[['user_id', 'target_user_id']])).any(axis = 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this reflects the number of user - target_user likes where the user _skipped_ the target_user one or more times during daily recommendation. We want to remove these from the dislikes data too, to avoid complication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [df, dislikes]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=df[(df[['user_id', 'target_user_id']].duplicated(keep=False)) & (df.liked == 0)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.liked.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Score and Applying Penalties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting for Likes and Matches\n",
    "\n",
    "The intuition here is that users who like less often probably are more careful in their liking, and so fewer likes per active user suggests more value for learning from that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_likes_weighting = user_like_counts/(1 + np.log(user_like_counts))\n",
    "received_likes_weighting = target_like_counts/(1 + np.log(target_like_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_like_counts, target_like_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(sent_likes_weighting, on = 'user_id', how = 'left')\n",
    "df = df.join(received_likes_weighting, on = 'target_user_id', how = 'left')\n",
    "\n",
    "df = df.rename({'user_likes_sent': 'send_penalty', 'target_likes_received': 'receive_penalty'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matches are rare and presumably powerful indicators of mutual interest. \n",
    "### So, we take the inverse of the average, which winds up being like 10~33.0 (it gets added linearly).\n",
    "### Note, NaNs were introduced into \"matched\" by adding the dislikes data, but they are ignored by the mean() function.\n",
    "matching_weight = df.matched.mean(skipna = True)**-1\n",
    "matching_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.fillna(0)\n",
    "X['match_score'] = X.matched * matching_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating and Standardizing our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_data = X[X.gender == 1].copy()\n",
    "female_data = X[X.gender == 2].copy()\n",
    "\n",
    "del X, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standardized_interaction_matrix(X):\n",
    "    X['has_greeting_score'] = X.has_greeting * (X.has_greeting.mean()**-0.5)\n",
    "    for col in ['has_greeting_score', 'match_score', 'receive_penalty', 'send_penalty']:\n",
    "        X[col] = (X[col] - X[col].min())/(X[col].max() - X[col].min())\n",
    "    ### Rating as linear combination of standardized, weighted features\n",
    "    X['rating'] = X.has_greeting_score + X.match_score + X.receive_penalty + X.send_penalty\n",
    "    \n",
    "    ### and now, setting rating to -1 if the target has been passed on\n",
    "    X['rating'] = X.apply(lambda x: -1 if x['liked'] == 0 else x['rating'], axis = 1)\n",
    "    \n",
    "    return X[['user_id', 'target_user_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_data = create_standardized_interaction_matrix(male_data)\n",
    "female_data = create_standardized_interaction_matrix(female_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_data.rating.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_data.rating.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsifying Our Data for Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2mat(df, m, n):\n",
    "    mat = csr_matrix(\n",
    "        (df[\"rating\"], (m, n)),\n",
    "        shape = (m.max()+1, n.max()+1),\n",
    "        dtype=np.float32)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_uid_codes = {uid:code for uid, code in zip(male_data.user_id.values, male_data.user_id.astype('category').cat.codes)}\n",
    "male_tid_codes = {code:tid for tid, code in zip(male_data.target_user_id.values, male_data.target_user_id.astype('category').cat.codes)}\n",
    "male_tid_codes_inv = {tid:code for tid, code in zip(male_data.target_user_id.values, male_data.target_user_id.astype('category').cat.codes)}\n",
    "\n",
    "female_uid_codes = {uid:code for uid, code in zip(female_data.user_id.values, female_data.user_id.astype('category').cat.codes)}\n",
    "female_tid_codes = {code:tid for tid, code in zip(female_data.target_user_id.values, female_data.target_user_id.astype('category').cat.codes)}\n",
    "female_tid_codes_inv = {tid:code for tid, code in zip(female_data.target_user_id.values, female_data.target_user_id.astype('category').cat.codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_sparse = df2mat(male_data, male_data.user_id.astype('category').cat.codes, male_data.target_user_id.astype('category').cat.codes)\n",
    "F_sparse = df2mat(female_data, female_data.user_id.astype('category').cat.codes, female_data.target_user_id.astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a few utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_items(user_id):\n",
    "    \"\"\"Filter out target users who meet any of the following conditions: \n",
    "    -> user and targets have different district_id\n",
    "    -> user and targets' user ages are more than 5 years apart\n",
    "    \"\"\"\n",
    "    tmp_age = likes[likes.user_id == user_id].user_age.iloc[0]\n",
    "    tmp_gender = likes[likes.user_id == user_id].gender.iloc[0]\n",
    "    tmp_district_id = likes[likes.user_id == user_id].user_district_id.iloc[0]\n",
    "    likes_subset = likes[likes.target_gender != tmp_gender]\n",
    "    filtered_target_users = likes_subset.query(\" \\\n",
    "                                        (5 < abs({} - target_user_age) or ({} != target_district_id)) \\\n",
    "    \".format(tmp_age, tmp_district_id)).target_user_id.values\n",
    "    \n",
    "    return set(filtered_target_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(users, model, X_sparse, uid_codes, tid_codes, tid_codes_inv, N = 100):\n",
    "    recommendations = {}\n",
    "    for user in users:\n",
    "        to_filter = [tid_codes_inv[t_user] for t_user in filter_items(user) if t_user in tid_codes_inv]\n",
    "        uid_code = uid_codes[user]\n",
    "        try:\n",
    "            recs = model.recommend(\n",
    "                uid_code, \n",
    "                X_sparse, \n",
    "                N = N, \n",
    "                filter_already_liked_items=True,\n",
    "                filter_items=to_filter\n",
    "            )\n",
    "            recs = [(tid_codes[r[0]], r[1]) for r in recs]\n",
    "            recommendations[user] = recs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return recommendations\n",
    "\n",
    "def get_rec_counts(recs):\n",
    "    rec_counts = {}\n",
    "    for uid, recs in recs.items():\n",
    "        for rec in recs:\n",
    "            rec_counts[rec[0]] = rec_counts.get(rec[0], 0) + 1\n",
    "    return rec_counts\n",
    "\n",
    "def convert_to_long(all_recs):\n",
    "    for user, recs in all_recs.items():\n",
    "        for rec in recs:\n",
    "            yield int(user), int(rec[0]), rec[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Bayesian Optimizaton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPR_m_model = implicit.bpr.BayesianPersonalizedRanking()\n",
    "BPR_m_model.fit(M_sparse.T.tocoo())\n",
    "BPR_f_model = implicit.bpr.BayesianPersonalizedRanking()\n",
    "BPR_f_model.fit(F_sparse.T.tocoo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPR_male_recs = generate_recommendations(\n",
    "    users=male_data.user_id.unique(), \n",
    "    model=BPR_m_model, \n",
    "    X_sparse=M_sparse,\n",
    "    uid_codes=male_uid_codes,\n",
    "    tid_codes=male_tid_codes,\n",
    "    tid_codes_inv=male_tid_codes_inv,\n",
    "    N=N_recs\n",
    ")\n",
    "\n",
    "BPR_female_recs = generate_recommendations(\n",
    "    users=female_data.user_id.unique(), \n",
    "    model=BPR_f_model, \n",
    "    X_sparse=F_sparse,\n",
    "    uid_codes=female_uid_codes,\n",
    "    tid_codes=female_tid_codes,\n",
    "    tid_codes_inv=female_tid_codes_inv,\n",
    "    N=N_recs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPR_m_rec_counts = get_rec_counts(BPR_male_recs)\n",
    "BPR_f_rec_counts = get_rec_counts(BPR_female_recs)\n",
    "\n",
    "BPR_m_rec_cov = len(BPR_m_rec_counts)/male_data.target_user_id.nunique()\n",
    "BPR_f_rec_cov = len(BPR_f_rec_counts)/female_data.target_user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coverage proportion for males:   {}\".format(BPR_m_rec_cov))\n",
    "print(\"Coverage proportion for females: {}\".format(BPR_f_rec_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(BPR_f_rec_counts))),sorted(list(BPR_f_rec_counts.values()), reverse = True))\n",
    "plt.plot(list(range(len(BPR_m_rec_counts))),sorted(list(BPR_m_rec_counts.values()), reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Out Combined Predictions\n",
    "Of dimensions user_ids x target_user_id (long and thin matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_recs_df_long = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            convert_to_long(BPR_male_recs), \n",
    "            columns = ['user_id', 'target_user_id', 'predicted_rating']\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            convert_to_long(BPR_female_recs),\n",
    "            columns = ['user_id', 'target_user_id', 'predicted_rating']\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_recs_df_long.groupby('target_user_id').user_id.count().plot(kind = 'hist', bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_recs_bucket(recs_df):\n",
    "    n_limit = min(N_recs, recs_df.groupby(\"user_id\").target_user_id.count().max())\n",
    "    recs_df = recs_df.groupby(\"user_id\").apply(\n",
    "        lambda x: x.sample(n = n_limit, replace = False) if len(x) >= n_limit else x\n",
    "    ).reset_index(drop = True)\n",
    "    recs_df.to_csv(out_bucket + outfile, header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_recs_bucket(combined_recs_df_long);"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}